# Single-Node Patterns

You might want to break up the components running on a single machine into different containers.

In general, the goal of a container is to establish **boundaries** around specific resources (e.g., this application 
needs two cores and 8 GB of memory). Likewise, the boundary delineates team ownership (e.g., this team owns this 
image). Finally, the boundary is intended to provide separation of concerns (e.g., this image does this one thing).

In contrast to multi-node, distributed patterns, all of these patterns in this session assume tight dependencies among 
all of the containers in the pattern. In particular, they assume that all of the containers in the pattern can be reliably 
coscheduled onto a single machine. They also assume that all of the containers in the pattern can optionally share 
volumes or parts of their filesystems as well as other key container resources like network namespaces and shared 
memory. This tight grouping is called **a** pod in Kubernetes.

## 1. The Sidecar Pattern

The sidecar pattern is a single-node pattern made up of two containers. The first is the **application container**. It 
contains the core logic for the application. Without this container, the application would not exist. In addition to 
the application container, there is a **sidecar container**. The role of the sidecar is to augment and improve the 
application container, often without the application containerâ€™s knowledge. In its simplest form, a sidecar 
container can be used to add functionality to a container that might otherwise be difficult to improve. Sidecar 
containers are coscheduled onto the same machine via an atomic container group, such as the pod API object in 
Kubernetes. In addition to being scheduled on the same machine, the application container and sidecar container 
share a number of resources, including parts of the filesystem, hostname and network, and many other namespaces.

<img width="694" alt="image" src="https://user-images.githubusercontent.com/47337188/239130606-e1744ad8-bae9-45e3-af92-c3490585a9f4.png">

The ambassadors pattern and the adapter pattern below inherit the generic sidecar pattern, with different purposes 
and focuses.

### Use Case - Dynamic Configuration

Many applications use a configuration file for parameterizing the application; this may be a raw text file or 
something more structured like XML, JSON, or YAML. Many pre-existing applications were written to assume that this 
file was present on the filesystem and read their configuration from there. However, in a cloud-native environment 
it is often quite useful to use an API for updating configuration. This allows you to do a dynamic push of 
configuration information via an API instead of manually logging in to every server and updating the configuration 
file using imperative commands. The desire for such an API is driven both by ease of use as well as the ability to 
add automation like rollback, which makes configuring (and reconfiguring) safer and easier.

new applications can be written with the expectation that configuration is a dynamic property that should be 
obtained using a cloud API, but adapting and updating an existing application can be significantly more challenging. 
Fortunately, the sidecar pattern again can be used to provide new functionality that augments a legacy application 
without changing the existing application.

When the legacy application starts, it loads its configuration from the filesystem, as expected. When the 
configuration manager starts, it examines the configuration API and looks for differences between the local 
filesystem and the configuration stored in the API. If there are differences, the configuration manager downloads 
the new configuration to the local filesystem and signals to the legacy application that it should reconfigure 
itself with this new configuration. The actual mechanism for this notification varies by application.

<img width="684" alt="image" src="https://user-images.githubusercontent.com/47337188/239132701-86e67895-1515-4eea-92a5-9f5a49ad0164.png">

### Use Case - Modular Application Containers

One of the other main advantages of using the sidecar pattern is modularity and reuse of the components used as 
sidecars. In deploying any real-world, reliable application, there is functionality that you need for debugging or 
other management of the application, such as giving a readout of all of the different processes using resources in 
the container, similar to the top command line tool.

This topz functionality can be deployed as a sidecar container that shares the process-id (PID) namespace with the 
application container. This topz container can introspect all running processes and provide a consistent user 
interface. Moreover, you can use the orchestration system to automatically add this container to all applications 
deployed via the orchestration system to ensure that there is a consistent set of tools available for all 
applications running in your infrastructure.

### Designing Sidecars for Modularity and Reusability

In all of the examples of sidecars that we have detailed throughout this chapter, one of the most important themes is that every one was a modular, reusable artifact. To be successful, the sidecar should be reusable across a wide variety of applications and deployments. By achieving modular reuse, sidecars can dramatically speed up the building of your application.

However, this modularity and reusability, just like achieving modularity in high-quality software development requires focus and discipline. In particular, you need to focus on developing three areas:

- Parameterizing your containers

Consider your container as a function in your program. How many parameters does it have? Each parameter represents 
an input that can customize a generic container to a specific situation.

Now that we know the parameters we want to expose, how do we actually expose them to users, and how do we consume 
them inside the container. There are two ways in which such parameters can be passed to your container: through 
environment variables or the command line. The other part is actually using these variables inside the container. 
Typically, to do that, a simple shell script is used that loads the environment variables supplied with the sidecar 
container and either adjusts the configuration files or parameterizes the underlying application.

- Creating the API surface of your container

As you think about defining modular, reusable containers, it is important to realize that all aspects of how your 
container interacts with its world are part of the API defined by that reusable container.

- Documenting the operation of your container

For every container image, the most obvious place to look for documentation is the Dockerfile from which the 
container was built. There are some parts of the Dockerfile that already document how the container works.

## 2. The Ambassadors Pattern

<img width="704" alt="image" src="https://user-images.githubusercontent.com/47337188/239137561-56be5914-8674-463d-807e-94ede7c4fb38.png">

The ambassador pattern - where an ambassador container brokers interactions between the application container and the 
rest of the world.

### Use Case - Shard a Service

When adapting an existing application to a sharded backend, you can introduce an ambassador container that contains 
all of the logic needed to route requests to the appropriate storage shard. Thus, your frontend or middleware 
application only connects to what appears to be a single storage backend running on localhost. However, this server 
is in fact actually a sharding ambassador proxy, which receives all of the requests from your application code, 
sends a request to the appropriate storage shard, and then returns the result to your application.

The net result of applying the ambassador pattern to sharded services is a separation of concerns between the 
application container, which simply knows it needs to talk to a storage service and discovers that service on 
localhost, and the sharding ambassador proxy, which only contains the code necessary to perform appropriate sharding.

### Use Case - Service Brokering

When trying to render an application portable across multiple environments (e.g., public cloud, physical datacenter, 
or private cloud), one of the primary challenges is service discovery and configuration. To understand what this 
means, imagine a frontend that relies on a MySQL database to store its data. In the public cloud, this MySQL service 
might be provided as software-as-a-service (SaaS), whereas in a private cloud it might be necessary to dynamically 
spin up a new virtual machine or container running MySQL.

Consequently, building a portable application requires that the application know how to introspect its environment 
and find the appropriate MySQL service to connect to. This process is called service discovery, and the system that 
performs this discovery and linking is commonly called a **service broker**.

As with previous examples, the ambassador pattern enables a system to separate the logic of the application 
container from the logic of the service broker ambassador. The application simply always connects to an instance of 
the service (e.g., MySQL) running on localhost. It is the responsibility of the service broker ambassador to 
introspect its environment and broker the appropriate connection.

## 3. The Adapters Pattern

In the adapter pattern, the adapter container is used to modify the **interface** of the application container so that 
it conforms to some predefined interface that is expected of all applications. For example, an adapter might ensure 
that an application implements a consistent monitoring interface. Or it might ensure that log files are always 
written to stdout or any number of other conventions.

To effectively monitor and operate your application, you need common interfaces. When each application provides 
metrics using a different format and interface, it is very difficult to collect all of those metrics in a single 
place for visualization and alerting. This is where the adapter pattern is relevant. Like other single-node patterns,
the adapter pattern is made up of modular containers. Different application containers can present many different 
monitoring interfaces while the adapter container adapts this heterogeneity to present a consistent interface. This 
enables you to deploy a single tool that expects this single interface.

<img width="690" alt="image" src="https://user-images.githubusercontent.com/47337188/239136442-a8356b99-75d8-4fcd-8c8a-f7df4050a384.png">

### Use Case - Monitoring

The adapter container contains the tools for transforming the monitoring interface exposed by the application 
container into the interface expected by the general-purpose monitoring system.

As an example, consider monitoring your containers via the Prometheus open source project. Prometheus is a monitoring aggregator, which collects metrics and aggregates them into a single time-series database. On top of this database, Prometheus provides visualization and query language for introspecting the collected metrics. To collect metrics from a variety of different systems, Prometheus expects every container to expose a specific metrics API. This enables Prometheus to monitor a wide variety of different programs through a single interface.

However, many popular programs, such as the Redis key-value store, do not export metrics in a format that is 
compatible with Prometheus. Consequently, the adapter pattern is quite useful for taking an existing service like 
Redis and adapting it to the Prometheus metrics-collection interface.

### Use Case - Logging

Much like monitoring, there is a wide variety of heterogeneity in how systems log data to an output stream. Systems 
might divide their logs into different levels (such as debug, info, warning, and error) with each level going into a 
different file. Some might simply log to stdout and stderr. This is especially problematic in the world of 
containerized applications where there is a general expectation that your containers will log to stdout, because 
that is what is available via commands like docker logs or kubectl logs.

Adding further complexity, the information logged generally has structured information (e.g., the date/time of the 
log), but this information varies widely between different logging libraries.

Of course, when you are storing and querying the logs for your distributed system, you donâ€™t really care about these 
differences in logging format. You want to ensure that despite different structures for the data, every log ends up 
with the appropriate timestamp.

Fortunately, as with monitoring, the adapter pattern can help provide a modular, re-usable design for both of these 
situations. While the application container may log to a file, the adapter container can redirect that file to 
stdout. Different application containers can log information in different formats, but the adapter container can 
transform that data into a single structured representation that can be consumed by your log aggregator.

# References

Designing Distributed Systems By Brendan Burns

# Retrieval Augmented Generation(RAG)

## What

The RAG model combines a language model with an information retrieval system to improve performance on 
knowledge-intensive tasks. It uses a **retriever** to find relevant information and a **generator** to produce accurate and 
detailed responses. This approach enhances the accuracy and flexibility of language models by allowing them to 
**access** and use **up-to-date** external information.

## Why

Traditional language models like GPT store knowledge in their _parameters*_ but struggle to update knowledge and 
sometimes produce incorrect information (hallucinations).

Solution: By adding a retrieval component, the model can look up relevant information from a large database (like 
Wikipedia) to generate more accurate and up-to-date responses.

_*parameters_: Parameters in machine learning models are the internal variables that the model learns from the 
training data. These parameters help the model make predictions or generate outputs based on new inputs. 	In the 
context of language models, these parameters capture patterns, relationships, and knowledge from the text data they 
are trained on.

## How RAG works
- Two Main Parts
  - Retriever: Finds relevant documents based on the input question.
  - Generator: Uses these documents to help generate a detailed and accurate response.
- Viz (in paper)
  - Query Encoder: Turns the input question into a format the retriever can use.
  - Retriever: Searches a large database for relevant documents.
  - Document Index: The database of documents (e.g., Wikipedia).
  - Generator: Uses the input question and retrieved documents to generate the final answer.
  
<img width="962" alt="image" src="https://gist.github.com/assets/47337188/16466198-d16a-4af0-ae6b-b2b8e6985400.jpg">


_*Parametric Memory_: Knowledge stored within the model’s parameters.

_*Non-Parametric Memory_: External knowledge sources, like databases or documents, that the model can look up.

## Types of RAG Models

The final probability is a product of summed likelihoods for each token, considering the top-K documents’ relevance 
to the query for each token individually.

- RAG-Sequence
  - The entire response is generated based on each of the top-K retrieved documents.
  - The final probability is a weighted sum of the likelihoods from each document, taking into account their 
	relevance to the query.
- RAG-Token
  - Each token in the response can be influenced by different retrieved documents.
  - The final probability is a product of summed likelihoods for each token, considering the top-K documents’ 
	relevance to the query for each token individually.

Use cases:
- RAG-Sequence:
  - Best For: Simple, direct questions with answers typically found in a single document.
  - Stength: Coherent context from a single source.
  - Use Cases: Simple factual questions, clear source of truth scenarios.
- RAG-Token:
  - Best For: Complex questions needing comprehensive answers from multiple sources. 
  - Stength: High precision by combining information from various documents.
  - Use Cases: Detailed explanations, multi-faceted questions, high-precision tasks.

## Example RAG Application

<img width="798" alt="image" src="https://gist.github.com/assets/47337188/a0ed8b38-2b79-4073-8bb6-ef8e9eab6143.jpg">

RAG applications require a pipeline and a chain component to perform the following:

- Indexing: A pipeline that ingests data from a source and indexes it. This data can be structured or unstructured.
- Retrieval and generation: This is the actual RAG chain. It takes the user query and retrieves similar data from the 
index, then passes the data, along with the query, to the LLM model.

The following describes the details of the RAG chain in the context of an unstructured data RAG example.


![image](https://gist.github.com/assets/47337188/dbb349f8-f30b-49e5-9a02-e2efe8fa10a3.jpg)


1. Embed the request using the same _embedding model*_ that was used to embed the data in the knowledge base.
2. Query the _vector database*_ to do a similarity search between the embedded request and the embedded data chunks in 
the vector database.
3. Retrieve the data chunks that are most relevant to the request.
4. Feed the relevant data chunks and the request to a customized LLM. The data chunks provide context that helps the 
LLM generate an appropriate response. Often, the LLM has a template for how to format the response.
5. Generate a response.

_*Vector Database_: A vector database indexes and stores vector embeddings for fast retrieval and similarity search.

<img width="981" alt="image" src="https://gist.github.com/assets/47337188/8c91df7d-2d62-4a9c-be02-fc495a9cc069.jpg">

<img width="948" alt="image" src="https://gist.github.com/assets/47337188/26c049f2-9bf6-4890-8cbf-9da776a2b598.jpg">

_*embedding model_:
When both the question and the documentation are embedded using the same model, they are represented in the same 
vector space. This means their embeddings (vector representations) can be directly compared.

## References

https://arxiv.org/pdf/2005.11401

https://docs.databricks.com/en/generative-ai/retrieval-augmented-generation.html

What is Retrieval-Augmented Generation (RAG)?
https://www.youtube.com/watch?v=T-D1OfcDW1M

Vector Databases simply explained! (Embeddings & Indexes)
https://www.youtube.com/watch?v=dN0lsF2cvm4
# Data Transformation In Practice

## Data Storage and Management Systems

| Feature / Concept | Relational Databases | NoSQL Databases | Data Warehouses | Data Lakes |
|-------------------|----------------------|-----------------|-----------------|------------|
| **Data Types** | Structured data (tables with rows and columns). | Structured, semi-structured, and unstructured data. | Primarily structured data, optimized for analytics. | Structured, semi-structured, and unstructured data in raw form. |
| **Schema** | Schema-on-Write (fixed schema). | Schema-on-Read (flexible schema). | Schema-on-Write (fixed schema, often complex for analytics). | Schema-on-Read (no predefined schema). |
| **Use Cases** | OLTP (Online Transaction Processing), real-time data access, and management. | Large-scale, flexible applications; real-time analytics; IoT; content management. | OLAP (Online Analytical Processing), business intelligence, reporting, and complex queries. | Big data analytics, machine learning, data discovery, and historical data analysis. |
| **Scalability** | Vertically scalable (limited compared to NoSQL). | Highly scalable, both horizontally and vertically. | Scalable but can be expensive; cloud-based solutions offer more flexibility. | Extremely scalable, especially in cloud-based environments. |
| **Query Language** | SQL (Structured Query Language). | Varies (document, key-value, graph, column-family models). | SQL and extensions for analytics. | Requires big data processing tools (e.g., Hadoop, Spark) for querying. |
| **Data Integrity** | High (ACID properties). | Varies; some support ACID transactions, others prioritize performance and scalability. | High (ACID properties, especially in traditional setups). | Not typically focused on ACID properties; more about storage and analysis. |
| **Performance** | Optimized for transactional consistency and integrity. | Optimized for high throughput and flexibility. | Optimized for complex queries and aggregations. | Optimized for large-scale data processing and analytics. |
| **Storage Cost** | Moderate; depends on infrastructure. | Generally cost-effective due to flexibility in data storage. | Higher due to structured nature and processing capabilities. | Lower for raw data storage; costs associated with processing large datasets. |

## ETL

ETL (Extract, Transform, Load) processes are central to managing and utilizing data across various systems such as 
Relational Databases, NoSQL Databases, Data Warehouses, and Data Lakes. In this context, ETL involves **extracting** 
data from diverse sources, which can range from structured data in relational databases to unstructured data in 
NoSQL systems. The **transformation** step adapts this data into a suitable format, which might include cleaning, 
normalizing, and structuring the data, particularly to align with the analytical requirements of Data Warehouses or 
the flexible schema-on-read approach of Data Lakes. Finally, the **load** phase involves transferring the processed data 
to a chosen storage system - often a Data Warehouse for structured, analytical queries or a Data Lake for 
large-scale, raw data storage and future processing. Each system interacts with ETL processes in a way that reflects 
its unique strengths and roles in data management and analytics.

## T(ransformation) Types

- **Basic Transformations**: Normalization, standardization, type conversion, and simple mathematical transformations.
- **Complex Transformations**: Pivot, transpose, and applying business logic.

Below is an example to demonstrate basic/complex transformations.
```shell
>>> wide_df = spark.createDataFrame([(1, 11, 1.1), (2, 12, 1.2)], ['id', 'int', 'double'])
>>> wide_df.show()
+---+---+------+
| id|int|double|
+---+---+------+
|  1| 11|   1.1|
|  2| 12|   1.2|
+---+---+------+
>>> long_df = wide_df.unpivot(ids='id', values=['int', 'double'], variableColumnName='type', valueColumnName='value')
>>> long_df.show()
+---+------+-----+
| id|  type|value|
+---+------+-----+
|  1|   int| 11.0|
|  1|double|  1.1|
|  2|   int| 12.0|
|  2|double|  1.2|
+---+------+-----+
>>> new_wide_df = long_df.groupBy('id').pivot(pivot_col='type', values=['int', 'double']).agg(first('value'))
>>> new_wide_df.show()
+---+----+------+
| id| int|double|
+---+----+------+
|  1|11.0|   1.1|
|  2|12.0|   1.2|
+---+----+------+
>>> new_wide_df.withColumn('int', col('int').cast('int')).show()
+---+---+------+
| id|int|double|
+---+---+------+
|  1| 11|   1.1|
|  2| 12|   1.2|
+---+---+------+
>>> wide_df.pandas_api().transpose().to_spark(index_col='new_id').show()
+------+----+----+
|new_id|   0|   1|
+------+----+----+
|    id| 1.0| 2.0|
|   int|11.0|12.0|
|double| 1.1| 1.2|
+------+----+----+
```
  

Define `df` as shown below to be used in the following examples.

```shell
>>> df = spark.createDataFrame([(1, 2), (3, 4), (5, None)], ["A", "B"])
>>> df.show()
+---+----+
|  A|   B|
+---+----+
|  1|   2|
|  3|   4|
|  5|NULL|
+---+----+
```

- **Aggregation**: Summarizing data (sums, averages, counts, max, min).

```shell
>>> df.agg({'A': 'sum', 'B': 'avg'}).show()
+------+------+
|sum(A)|avg(B)|
+------+------+
|     9|   3.0|
+------+------+
```

- **Filtering**: Removing unnecessary or irrelevant data.

```shell
>>> df.filter(df['A'] > 1).show()
+---+----+
|  A|   B|
+---+----+
|  3|   4|
|  5|NULL|
+---+----+
```

- **Joining and Merging**: Combining data from different sources.

```shell
>>> other_df = spark.createDataFrame([(1, 'a'), (2, 'b'), (3, 'c')], ['A', 'C'])
>>> other_df.show()
+---+---+
|  A|  C|
+---+---+
|  1|  a|
|  2|  b|
|  3|  c|
+---+---+

>>> joined_df = df.join(other_df, 'A')
>>> joined_df.show()
+---+---+---+
|  A|  B|  C|
+---+---+---+
|  1|  2|  a|
|  3|  4|  c|
+---+---+---+
```

- **Data Cleaning**: Handling missing values, smoothing noisy data, removing outliers, resolving inconsistencies.

```shell
>>> df.na.fill({'B': -1}).show()
+---+---+
|  A|  B|
+---+---+
|  1|  2|
|  3|  4|
|  5| -1|
+---+---+
```
